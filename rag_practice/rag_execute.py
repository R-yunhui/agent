from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_core.runnables import RunnableConfig, RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import MessagesPlaceholder
from _collections_abc import Iterator
from langchain_core.runnables.utils import Output
from basic.embedding.custom_embeddings import CustomMultimodalEmbeddings
from langchain_core.embeddings import Embeddings
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility, db
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter

import os

import yaml
from typing import Dict

from langchain_openai.chat_models.base import BaseChatOpenAI

from config.llm_config import EmbeddingConfig, LLMConfig, RAGConfig, MilvusConfig, TextSplitterConfig, DocumentConfig


# ============================================================
# 1. åŠ è½½é…ç½®æ–‡ä»¶
# ============================================================

def load_config():
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.getcwd(), "config.yaml")
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


config = load_config()

# å­˜å‚¨å†å²ä¼šè¯è®°å½•ï¼Œåç»­ç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€‚ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨æ•°æ®åº“å­˜å‚¨ã€‚
chat_memory_history: Dict[str, ChatMessageHistory] = {}


def chat_with_memory(user_question: str, session_id: str) -> Iterator[Output]:
    """
    ä¸OpenAIæ¨¡å‹è¿›è¡Œä¸€æ¬¡å¯¹è¯ï¼Œè¿”å›æ¨¡å‹çš„å›å¤ã€‚
    è¯¥å‡½æ•°ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€‚

    :param user_question: ç”¨æˆ·çš„é—®é¢˜æˆ–æŒ‡ä»¤
    :param session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    :return: æ¨¡å‹çš„å›å¤å†…å®¹
    """
    # 1.å…ˆé€šè¿‡ rag è¿›è¡Œæ£€ç´¢
    vectorstore = create_vector_store()
    vectorstore_results = vectorstore.similarity_search_with_score(
        query=user_question,
        k=RAGConfig.RETRIEVAL_TOP_K,
    )

    message_history = create_chat_with_memory()

    if vectorstore_results is None or len(vectorstore_results) == 0:
        print("æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£, ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå›å¤")
    else:
        print(f"æ£€ç´¢åˆ° {len(vectorstore_results)} ä¸ªç›¸å…³æ–‡æ¡£")
        contents = []
        for result in vectorstore_results:
            document, score = result
            contents.append(document.page_content)
        else:
            user_question = f"""
                        è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
                        {contents}
                        ç”¨æˆ·é—®é¢˜ï¼š{user_question}
                        """
            print(f"æœ€æ–°çš„ç”¨æˆ·é—®é¢˜: {user_question}")

    return message_history.stream({"user_question": user_question}, config=RunnableConfig(
        configurable={"session_id": session_id}
    ))


def create_chat_with_memory() -> RunnableWithMessageHistory:
    """åˆ›å»ºä¸€ä¸ªå¯è®°å¿†å†å²çš„èŠå¤©å‡½æ•°"""
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", config["prompts"]["system"]),
        # å¯¹è¯å†å²
        MessagesPlaceholder(variable_name="history"),
        ("human", "{user_question}"),
    ])

    llm = get_large_model()
    runnable = prompt_template | llm

    message_history = RunnableWithMessageHistory(
        runnable=runnable,
        get_session_history=get_memory_history,
        input_messages_key="user_question",
        history_messages_key="history",
    )

    return message_history


def get_memory_history(session_id: str) -> ChatMessageHistory:
    """
    è·å–ä¼šè¯å†å²è®°å½•

    :param session_id: ä¼šè¯IDï¼Œç”¨äºå”¯ä¸€æ ‡è¯†ä¸€ä¸ªä¼šè¯
    :return: ä¼šè¯å†å²è®°å½•å¯¹è±¡ï¼ŒåŒ…å«è¯¥ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯
    """
    memory_history = chat_memory_history.get(session_id)
    if not memory_history:
        memory_history = ChatMessageHistory()
        chat_memory_history[session_id] = memory_history
    return memory_history


def get_large_model() -> ChatOpenAI:
    """è·å–å¤§æ¨¡å‹å®ä¾‹"""
    return ChatOpenAI(
        base_url=config["llm"]["base_url"],
        api_key=config["llm"]["api_key"],
        model=config["llm"]["model"],
        temperature=config["llm"]["temperature"],
        max_tokens=config["llm"]["max_tokens"],
    )


def rag_execute_with_file(path: str, session_id: str) -> Milvus:
    """
    æ‰§è¡ŒRAGæ¨¡å‹çš„ä¸€æ¬¡å¯¹è¯ï¼Œè¿”å›æ¨¡å‹çš„å›å¤ã€‚

    :param path: æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶
    :param session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    :return: æ¨¡å‹çš„å›å¤å†…å®¹
    """
    print(f"ç”¨æˆ·: {session_id}, å¼€å§‹å¤„ç†æ–‡ä»¶ç›®å½•: {path}")
    if not os.path.isdir(path):
        raise FileNotFoundError(f"æ–‡ä»¶ç›®å½• {path} ä¸å­˜åœ¨")

    file_list = os.listdir(path)
    if not file_list:
        raise FileNotFoundError(f"ç›®å½• {path} ä¸‹æ²¡æœ‰æ–‡ä»¶")

    documents = []
    for file in file_list:
        if not file.endswith(".txt"):
            continue

        try:
            with open(os.path.join(path, file), "r", encoding="utf-8") as f:
                for line in f:
                    documents.append(line)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
            continue

    # langchain å†…éƒ¨ä¼šè‡ªå·±åˆ›å»ºå’Œç®¡ç† milvus é“¾æ¥ï¼Œè‡ªå·±åˆ›å»º connection
    # from_texts ä¼šè‡ªåŠ¨å®Œæˆï¼šæ–‡æœ¬ -> å‘é‡åŒ– -> å­˜å‚¨
    embeddings = get_embedding_model()
    vectorstore = Milvus.from_texts(
        texts=documents,
        embedding=embeddings,
        collection_name=MilvusConfig.COLLECTION_NAME,
        connection_args={
            **MilvusConfig.get_connection_args(),
            "db_name": MilvusConfig.DB_NAME,
        },
        drop_old=True  # å¦‚æœé›†åˆå­˜åœ¨åˆ™åˆ é™¤ï¼Œæµ‹è¯•ä½¿ç”¨
    )

    print(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")

    # æ–­å¼€è¿æ¥
    connections.disconnect("default")
    return vectorstore


def create_vector_store():
    """
    åˆ›å»ºå‘é‡å­˜å‚¨
    :return: å‘é‡å­˜å‚¨å¯¹è±¡
    """
    return Milvus(
        collection_name=MilvusConfig.COLLECTION_NAME,
        connection_args={
            **MilvusConfig.get_connection_args(),
            "db_name": MilvusConfig.DB_NAME,
        },
        embedding_function=get_embedding_model(),
    )


def get_embedding_model() -> Embeddings:
    """
    è·å–è‡ªå®šä¹‰å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹å®ä¾‹

    :return: è‡ªå®šä¹‰å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹å¯¹è±¡
    """
    return CustomMultimodalEmbeddings(
        api_base=EmbeddingConfig.API_BASE,
        api_key=EmbeddingConfig.API_KEY,
        model=EmbeddingConfig.MODEL,
        batch_size=5
    )


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    session_id = "001"
    user_questions = ["pythonåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå¯ä»¥å¹²ä»€ä¹ˆï¼Ÿ"]
    for user_question in user_questions:
        response = chat_with_memory(user_question, session_id)
        for chunk in response:
            print(chunk.content, end="")
        else:
            print("\n")

    # path = os.path.join(os.getcwd(), "doc")
    # vectorstore = rag_execute_with_file(path, session_id)
    # if not vectorstore:
    #     raise Exception("å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥")
    # else:
    #     print("å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
    #     # è¿›è¡Œæ£€ç´¢æµ‹è¯•
    #     results_with_scores = vectorstore.similarity_search_with_score(
    #         query="æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œ",
    #         k=3
    #     )
    #
    #     print(f"æ£€ç´¢åˆ°çš„ç»“æœæ•°é‡: {len(results_with_scores)}")
    #     for j, (doc, score) in enumerate(results_with_scores, 1):
    #         print(f"\n  ç»“æœ {j}:")
    #         print(f"  ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
    #         print(f"  ğŸ“„ å†…å®¹: {doc.page_content}")


if __name__ == "__main__":
    main()
