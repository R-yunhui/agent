import os

from dotenv import load_dotenv
from typing import Any
from langchain_openai import ChatOpenAI
from langchain_community.tools import tool
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.runnables import RunnableConfig
from langchain.agents.middleware import AgentState, before_agent, after_agent, before_model, after_model
from langgraph.runtime import Runtime
from datetime import datetime

# åŠ è½½ç¯å¢ƒé…ç½®
load_dotenv()


@tool(description="è·å–å½“å‰æ—¶é—´")
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´"""
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")


@before_agent()
def before_agent_do(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"Before agent do: {state}")
    return None


@after_agent()
def after_agent_do(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"After agent do: {state}")
    return None


@before_model()
def before_model_do(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"Before model do: {state}")
    # æ ¡éªŒæ¶ˆæ¯çš„æ¢³ç†
    if state['messages']:
        print(f"æ¶ˆæ¯æ•°é‡: {len(state['messages'])}")
        # æœ€å¤šä¿ç•™ 10 æ¡æ•°æ®
        state['messages'] = state['messages'][-10:]
    return None


@after_model()
def after_model_do(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"After model do: {state}")
    return None


llm = ChatOpenAI(
    model=os.getenv("OPENAI_CHAT_MODEL"),
    base_url=os.getenv("OPENAI_API_BASE_URL"),
    api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.7,
    max_retries=3,
    max_tokens=4096
)


def chat(question: str, thread_id: str) -> str:
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜è·å–å½“å‰æ—¶é—´ã€‚
    """

    middleware_list = [
        before_agent_do,
        after_agent_do,
        before_model_do,
        after_model_do,
    ]

    with SqliteSaver.from_conn_string("chat_memory.db") as checkpointer:
        agent = create_agent(
            model=llm,
            system_prompt=system_prompt,
            tools=[get_current_time],
            middleware=middleware_list,
            checkpointer=checkpointer,
            debug=False,
        )

        response = agent.invoke(
            input={"messages": [HumanMessage(content=question)]},
            config=RunnableConfig(configurable={"thread_id": thread_id})
        )
        return response["messages"][-1].content


def main():
    session_id = "user-session-123"
    print("ğŸ¤– èŠå¤©æœºå™¨äººå¯åŠ¨ï¼è¾“å…¥ 'exit' é€€å‡º")

    while True:
        question = input("\nğŸ‘¤ ä½ : ").strip()
        if question.lower() == "exit":
            print("ğŸ‘‹ å†è§ï¼")
            break

        try:
            response = chat(question, session_id)
            print(f"ğŸ¤– åŠ©æ‰‹: {response}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
