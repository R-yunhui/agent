"""
Deep Research Agent - åŸºäº LangGraph çš„å¤š Agent æ·±åº¦ç ”ç©¶ç³»ç»Ÿ (å¼‚æ­¥ç‰ˆ)

æ¶æ„:
    ç”¨æˆ·é—®é¢˜ â†’ Planner â†’ Researcher (å†…éƒ¨å¹¶è¡Œ) â†’ Synthesizer â†’ Reflector â†’ è¾“å‡ºæŠ¥å‘Š
                              â†‘__________________|  (ä¿¡æ¯ä¸è¶³æ—¶å›é€€)
                                        â†‘________________________|  (è´¨é‡ä¸åˆæ ¼æ—¶å›é€€)

å¼‚æ­¥ç­–ç•¥:
    - ä½¿ç”¨ asyncio + httpx å®ç°å¼‚æ­¥ç½‘ç»œè¯·æ±‚
    - ä½¿ç”¨ asyncio.gather() å®ç°å¹¶è¡Œæœç´¢
    - ä½¿ç”¨ llm.ainvoke() å®ç°å¼‚æ­¥ LLM è°ƒç”¨
"""

import asyncio
from typing import TypedDict, List, Literal
import json
import httpx
import os
import re
from pathlib import Path
from datetime import datetime

from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv

load_dotenv()

# ============================================================
# 1. åˆå§‹åŒ– LLM
# ============================================================

llm = ChatTongyi(
    model=os.getenv("TONGYI_MODEL"),
    api_key=os.getenv("DASHSCOPE_KEY"),
    base_url=os.getenv("DASHSCOPE_BASE_URL"),
)


# ============================================================
# 2. å®šä¹‰çŠ¶æ€ (State) - æ‰€æœ‰ Node å…±äº«çš„æ•°æ®ç»“æ„
# ============================================================


class ResearchState(TypedDict):
    """ç ”ç©¶çŠ¶æ€ï¼Œåœ¨å„ä¸ª Node ä¹‹é—´ä¼ é€’"""

    # è¾“å…¥
    original_question: str  # ç”¨æˆ·åŸå§‹é—®é¢˜

    # Planner è¾“å‡º
    sub_questions: List[str]  # åˆ†è§£åçš„å­é—®é¢˜åˆ—è¡¨

    # Researcher è¾“å‡º
    search_results: List[dict]  # æœç´¢ç»“æœ

    # Synthesizer è¾“å‡º
    draft_report: str  # æŠ¥å‘Šè‰ç¨¿

    # Reflector è¾“å‡º
    reflection_result: dict  # åæ€ç»“æœ {passed: bool, issues: [], action: str}
    reflection_count: int  # åæ€æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰

    # æœ€ç»ˆè¾“å‡º
    final_report: str  # æœ€ç»ˆæŠ¥å‘Š


# ============================================================
# 3. å·¥å…·å‡½æ•° (å¼‚æ­¥ç‰ˆ)
# ============================================================


async def web_search_async(query: str, count: int = 5) -> List[dict]:
    """å¼‚æ­¥è°ƒç”¨åšæŸ¥ API è¿›è¡Œç½‘é¡µæœç´¢"""
    url = "https://api.bochaai.com/v1/web-search"
    headers = {
        "Authorization": f"Bearer {os.getenv('BOCHA_API_KEY', 'sk-77103117515748ca9df587b606992aa4')}",
        "Content-Type": "application/json",
    }
    data = {
        "query": query,
        "freshness": "noLimit",
        "summary": True,
        "count": count,
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=data)
            if response.status_code == 200:
                json_response = response.json()
                if json_response.get("code") == 200 and json_response.get("data"):
                    webpages = (
                        json_response["data"].get("webPages", {}).get("value", [])
                    )
                    return [
                        {
                            "title": page.get("name", ""),
                            "url": page.get("url", ""),
                            "summary": page.get("summary", ""),
                            "site": page.get("siteName", ""),
                            "date": page.get("dateLastCrawled", ""),
                        }
                        for page in webpages
                    ]
    except Exception as e:
        print(f"æœç´¢å‡ºé”™: {e}")

    return []


# ============================================================
# 4. å®šä¹‰å„ä¸ª Node (Agent) - å¼‚æ­¥ç‰ˆ
# ============================================================


async def planner_node(state: ResearchState) -> dict:
    """
    Planner Node: å°†ç”¨æˆ·é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ (å¼‚æ­¥ç‰ˆ)
    """
    question = state["original_question"]

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„é—®é¢˜åˆ†è§£ä¸º 3-5 ä¸ªå…·ä½“çš„å­é—®é¢˜ï¼Œä»¥ä¾¿è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚

ç”¨æˆ·é—®é¢˜: {question}
å½“å‰æ—¶é—´: {datetime.now().strftime("%Y-%m-%d")}

è¦æ±‚:
1. å­é—®é¢˜åº”è¯¥å…·ä½“ã€å¯æœç´¢
2. å­é—®é¢˜åº”è¯¥è¦†ç›–é—®é¢˜çš„å„ä¸ªæ–¹é¢
3. æŒ‰ç…§é€»è¾‘é¡ºåºæ’åˆ—

è¯·ç›´æ¥è¾“å‡º JSON æ ¼å¼:
{{"sub_questions": ["å­é—®é¢˜1", "å­é—®é¢˜2", "å­é—®é¢˜3"]}}
"""

    # å¼‚æ­¥è°ƒç”¨ LLM
    response = await llm.ainvoke([HumanMessage(content=prompt)])

    try:
        # è§£æ JSON å“åº”
        content = response.content.strip()
        # å¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        result = json.loads(content)
        sub_questions = result.get("sub_questions", [question])
    except (json.JSONDecodeError, KeyError):
        # è§£æå¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸé—®é¢˜
        sub_questions = [question]

    print(f"[Planner] åˆ†è§£ä¸º {len(sub_questions)} ä¸ªå­é—®é¢˜:")
    for i, q in enumerate(sub_questions, 1):
        print(f"  {i}. {q}")

    return {
        "sub_questions": sub_questions,
        "search_results": [],
    }


async def researcher_node(state: ResearchState) -> dict:
    """
    Researcher Node: å¯¹æ‰€æœ‰å­é—®é¢˜è¿›è¡Œå¹¶è¡Œæœç´¢ç ”ç©¶ (å¼‚æ­¥ç‰ˆ)

    ä½¿ç”¨ asyncio.gather() å®ç°å¹¶è¡Œæœç´¢ï¼Œæ‰€æœ‰å­é—®é¢˜åŒæ—¶å‘èµ·è¯·æ±‚
    """
    sub_questions = state["sub_questions"]

    print(f"[Researcher] å¯åŠ¨ {len(sub_questions)} ä¸ªå¹¶è¡Œæœç´¢ä»»åŠ¡...")

    async def search_single_question(index: int, question: str) -> dict:
        """å¼‚æ­¥æœç´¢å•ä¸ªå­é—®é¢˜"""
        print(f"  [Task {index + 1}] æ­£åœ¨æœç´¢: {question}")
        results = await web_search_async(question, count=5)
        print(f"  [Task {index + 1}] å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
        return {
            "question": question,
            "question_index": index,
            "results": results,
            "timestamp": datetime.now().isoformat(),
        }

    # ä½¿ç”¨ asyncio.gather() å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
    tasks = [search_single_question(i, q) for i, q in enumerate(sub_questions)]
    search_results = await asyncio.gather(*tasks, return_exceptions=True)

    # è¿‡æ»¤æ‰å¼‚å¸¸ç»“æœå¹¶æ’åº
    valid_results = [r for r in search_results if isinstance(r, dict)]
    valid_results.sort(key=lambda x: x["question_index"])

    print(f"[Researcher] æ‰€æœ‰æœç´¢å®Œæˆï¼Œå…± {len(valid_results)} ä¸ªç»“æœ")

    return {
        "search_results": valid_results,
    }


async def synthesizer_node(state: ResearchState) -> dict:
    """
    Synthesizer Node: ç»¼åˆæ‰€æœ‰æœç´¢ç»“æœï¼Œç”Ÿæˆç ”ç©¶æŠ¥å‘Šè‰ç¨¿ (å¼‚æ­¥ç‰ˆ)
    æ³¨æ„: è¿™é‡Œç”Ÿæˆçš„æ˜¯è‰ç¨¿ï¼Œéœ€è¦ç»è¿‡ Reflector è¯„ä¼°åæ‰èƒ½ç¡®å®šæ˜¯å¦è¾“å‡º
    """
    print("[Synthesizer] æ­£åœ¨ç»¼åˆä¿¡æ¯ç”ŸæˆæŠ¥å‘Šè‰ç¨¿...")

    original_question = state["original_question"]
    search_results = state["search_results"]

    # æ„å»ºæœç´¢ç»“æœæ‘˜è¦
    results_summary = ""
    for i, sr in enumerate(search_results, 1):
        results_summary += f"\n### å­é—®é¢˜ {i}: {sr['question']}\n"
        for j, r in enumerate(sr["results"], 1):
            results_summary += f"- [{r['title']}]({r['url']}): {r['summary']}\n"

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œæ’°å†™ä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_question}
å½“å‰æ—¶é—´: {datetime.now().strftime("%Y-%m-%d")}

æœç´¢ç»“æœ:
{results_summary}

è¦æ±‚:
1. æŠ¥å‘Šç»“æ„æ¸…æ™°ï¼ŒåŒ…å«æ‘˜è¦ã€æ­£æ–‡åˆ†æã€ç»“è®ºå’Œå»ºè®®
2. å¼•ç”¨æ¥æºï¼Œæ ‡æ³¨ URL
3. æ•°æ®å’Œè§‚ç‚¹è¦æœ‰ä¾æ®
4. è¯­è¨€ä¸“ä¸šä½†æ˜“äºç†è§£
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®æŒ‡å‡ºå“ªäº›æ–¹é¢éœ€è¦æ›´å¤šç ”ç©¶

è¯·æ’°å†™ç ”ç©¶æŠ¥å‘Š:
"""

    # å¼‚æ­¥ç”ŸæˆæŠ¥å‘Šè‰ç¨¿
    response = await llm.ainvoke([HumanMessage(content=prompt)])

    print("[Synthesizer] æŠ¥å‘Šè‰ç¨¿ç”Ÿæˆå®Œæˆï¼Œç­‰å¾…è¯„ä¼°...")

    return {
        "draft_report": response.content,
    }


async def reflector_node(state: ResearchState) -> dict:
    """
    Reflector Node: è¯„ä¼°æŠ¥å‘Šè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ”¹è¿› (å¼‚æ­¥ç‰ˆ)
    """
    print("[Reflector] æ­£åœ¨è¯„ä¼°æŠ¥å‘Šè´¨é‡...")

    draft_report = state["draft_report"]
    original_question = state["original_question"]
    reflection_count = state.get("reflection_count", 0)

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç ”ç©¶æŠ¥å‘Šè¯„å®¡ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æŠ¥å‘Šçš„è´¨é‡ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_question}

æŠ¥å‘Šå†…å®¹:
{draft_report}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ† (1-10):
1. å®Œæ•´æ€§: æ˜¯å¦å®Œæ•´å›ç­”äº†ç”¨æˆ·é—®é¢˜?
2. å‡†ç¡®æ€§: æ•°æ®å’Œç»“è®ºæ˜¯å¦æœ‰æ¥æºæ”¯æ’‘?
3. æ·±åº¦: åˆ†ææ˜¯å¦æœ‰æ´å¯ŸåŠ›?
4. å¯è¯»æ€§: ç»“æ„æ˜¯å¦æ¸…æ™°?

è¯·è¾“å‡º JSON æ ¼å¼:
{{
    "scores": {{"å®Œæ•´æ€§": 8, "å‡†ç¡®æ€§": 7, "æ·±åº¦": 6, "å¯è¯»æ€§": 9}},
    "total": 30,
    "passed": true,
    "issues": ["é—®é¢˜1", "é—®é¢˜2"],
    "action": "pass"
}}

action å¯é€‰å€¼:
- "pass": è´¨é‡åˆæ ¼ï¼Œå¯ä»¥è¾“å‡º
- "research_more": ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦è¡¥å……ç ”ç©¶
- "rewrite": é€»è¾‘æˆ–ç»“æ„æœ‰é—®é¢˜ï¼Œéœ€è¦é‡å†™

æ³¨æ„: æ€»åˆ† >= 28 æ‰ç®—é€šè¿‡ (passed=true)
"""

    # å¼‚æ­¥è°ƒç”¨ LLM
    response = await llm.ainvoke([HumanMessage(content=prompt)])

    try:
        content = response.content.strip()
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        result = json.loads(content)
    except (json.JSONDecodeError, KeyError):
        # è§£æå¤±è´¥ï¼Œé»˜è®¤é€šè¿‡
        result = {"passed": True, "action": "pass", "issues": [], "total": 32}

    print(
        f"[Reflector] è¯„ä¼°ç»“æœ: æ€»åˆ† {result.get('total', 'N/A')}, é€šè¿‡: {result.get('passed', True)}"
    )

    # é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤šåæ€ 2 æ¬¡
    if reflection_count >= 2:
        result["passed"] = True
        result["action"] = "pass"
        print("[Reflector] è¾¾åˆ°æœ€å¤§åæ€æ¬¡æ•°ï¼Œå¼ºåˆ¶é€šè¿‡")

    return {
        "reflection_result": result,
        "reflection_count": reflection_count + 1,
    }


def should_continue_after_reflection(
    state: ResearchState,
) -> Literal["planner", "synthesizer", "output"]:
    """
    æ¡ä»¶è¾¹: æ ¹æ®åæ€ç»“æœå†³å®šä¸‹ä¸€æ­¥
    """
    result = state.get("reflection_result", {})
    action = result.get("action", "pass")

    if action == "research_more":
        print("[Router] éœ€è¦è¡¥å……ç ”ç©¶ï¼Œå›åˆ° Planner é‡æ–°è§„åˆ’")
        return "planner"
    elif action == "rewrite":
        print("[Router] éœ€è¦é‡å†™æŠ¥å‘Š")
        return "synthesizer"
    else:
        print("[Router] è´¨é‡åˆæ ¼ï¼Œè¾“å‡ºæŠ¥å‘Š")
        return "output"


async def output_node(state: ResearchState) -> dict:
    """
    Output Node: è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š (å¼‚æ­¥ç‰ˆ)
    åªæœ‰ç»è¿‡ Reflector è¯„ä¼°é€šè¿‡åæ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
    """
    final_report = state["draft_report"]

    # ç›´æ¥è¾“å‡ºå®Œæ•´æŠ¥å‘Š
    print()
    print("=" * 60)
    print("ğŸ“ ç ”ç©¶æŠ¥å‘Š")
    print("=" * 60)
    # print(final_report)
    print("=" * 60)

    return {
        "final_report": final_report,
    }


# ============================================================
# 5. æ„å»º LangGraph å·¥ä½œæµ
# ============================================================


def build_research_graph():
    """
    æ„å»ºç ”ç©¶å·¥ä½œæµå›¾ (å¼‚æ­¥ç‰ˆ)

    å·¥ä½œæµç¨‹:
    1. planner: åˆ†è§£é—®é¢˜ä¸ºå­é—®é¢˜
    2. researcher: å¹¶è¡Œæœç´¢æ‰€æœ‰å­é—®é¢˜ (ä½¿ç”¨ asyncio.gather)
    3. synthesizer: ç»¼åˆæ‰€æœ‰ç»“æœ
    4. reflector: è¯„ä¼°è´¨é‡
    5. output: è¾“å‡ºæŠ¥å‘Š
    """

    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(ResearchState)

    # æ·»åŠ èŠ‚ç‚¹ (å¼‚æ­¥èŠ‚ç‚¹å‡½æ•°)
    workflow.add_node("planner", planner_node)
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("synthesizer", synthesizer_node)
    workflow.add_node("reflector", reflector_node)
    workflow.add_node("output", output_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("planner")

    # æ·»åŠ è¾¹: ç®€å•çš„çº¿æ€§æµç¨‹
    workflow.add_edge("planner", "researcher")
    workflow.add_edge("researcher", "synthesizer")
    workflow.add_edge("synthesizer", "reflector")

    # æ¡ä»¶è¾¹: åæ€åçš„è·¯ç”±
    workflow.add_conditional_edges(
        "reflector",
        should_continue_after_reflection,
        {
            "planner": "planner",  # ä¿¡æ¯ä¸è¶³ï¼Œå›åˆ° planner é‡æ–°è§„åˆ’
            "synthesizer": "synthesizer",  # éœ€è¦é‡å†™æŠ¥å‘Š
            "output": "output",  # è´¨é‡åˆæ ¼ï¼Œè¾“å‡º
        },
    )

    workflow.add_edge("output", END)

    # ç¼–è¯‘å›¾
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)

    return app


# ============================================================
# 6. ä¸»å‡½æ•° (å¼‚æ­¥ç‰ˆ)
# ============================================================


async def deep_research(question: str, session_id: str = "default") -> str:
    """
    æ‰§è¡Œæ·±åº¦ç ”ç©¶ (å¼‚æ­¥ç‰ˆ)

    Args:
        question: ç”¨æˆ·é—®é¢˜
        session_id: ä¼šè¯ ID

    Returns:
        ç ”ç©¶æŠ¥å‘Š
    """
    print()
    print("â•" * 60)
    print("ğŸ” æ·±åº¦ç ”ç©¶ç³»ç»Ÿ (Async)")
    print("â•" * 60)
    print(f"ç”¨æˆ·é—®é¢˜: {question}")
    print("-" * 60)

    app = build_research_graph()

    # åˆå§‹çŠ¶æ€
    initial_state = {
        "original_question": question,
        "sub_questions": [],
        "search_results": [],
        "draft_report": "",
        "reflection_result": {},
        "reflection_count": 0,
        "final_report": "",
    }

    # å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµ
    config = {"configurable": {"thread_id": session_id}}
    final_report = ""

    async for event in app.astream(initial_state, config):
        for node_name, node_output in event.items():
            if node_name == "output":
                final_report = node_output.get("final_report", "")
                print()
                print("â•" * 60)
                print("âœ… ç ”ç©¶å®Œæˆ!")
                print("â•" * 60)

    return final_report


def save_report_to_markdown(
    report: str, question: str, output_dir: str = "reports"
) -> str:
    """
    å°†ç ”ç©¶æŠ¥å‘Šä¿å­˜ä¸º Markdown æ–‡ä»¶

    Args:
        report: æŠ¥å‘Šå†…å®¹
        question: ç”¨æˆ·é—®é¢˜ (ç”¨äºç”Ÿæˆæ–‡ä»¶å)
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶åï¼šæ—¶é—´æˆ³ + é—®é¢˜å‰20å­—ç¬¦
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # æ¸…ç†é—®é¢˜ä¸­çš„éæ³•æ–‡ä»¶åå­—ç¬¦
    safe_question = re.sub(r'[\\/:*?"<>|]', "", question)[:20].strip()
    filename = f"{timestamp}_{safe_question}.md"

    filepath = output_path / filename

    # æ„å»º Markdown å†…å®¹
    markdown_content = f"""# ç ”ç©¶æŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
> 
> ç ”ç©¶é—®é¢˜: {question}

---

{report}

---

*æœ¬æŠ¥å‘Šç”± Deep Research Agent è‡ªåŠ¨ç”Ÿæˆ*
"""

    # å†™å…¥æ–‡ä»¶
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {filepath}")

    return str(filepath)


if __name__ == "__main__":
    # ä½¿ç”¨ asyncio.run() å¯åŠ¨å¼‚æ­¥ä¸»å‡½æ•°
    question = """
    ç”Ÿæˆä¸€ä»½DeeepResearchæ™ºèƒ½ä½“ç«å“è°ƒç ”çš„æŠ¥å‘Š
    """
    report = asyncio.run(deep_research(question))

    # ä¿å­˜ä¸º Markdown æ–‡ä»¶
    if report:
        save_report_to_markdown(report, question)
