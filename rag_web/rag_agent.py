from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig, RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.embeddings import Embeddings
from langchain_openai.embeddings import OpenAIEmbeddings
from pymilvus import connections, db
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter

import os
import yaml
from typing import Dict, Iterator
from _collections_abc import Iterator as ABCIterator

from basic.embedding.custom_embeddings import CustomMultimodalEmbeddings


# ============================================================
# 1. åŠ è½½é…ç½®æ–‡ä»¶
# ============================================================

def load_config():
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "config/config.yaml")
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


config = load_config()

# å­˜å‚¨å†å²ä¼šè¯è®°å½•ï¼Œåç»­ç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€‚ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨æ•°æ®åº“å­˜å‚¨ã€‚
chat_memory_history: Dict[str, ChatMessageHistory] = {}

connection_args = {
    "uri": f"http://{config['milvus']['host']}:{config['milvus']['port']}",
    "user": config["milvus"]["user"],
    "password": config["milvus"]["password"]
}


def chat_with_memory(user_question: str, session_id: str) -> ABCIterator:
    """
    ä¸OpenAIæ¨¡å‹è¿›è¡Œä¸€æ¬¡å¯¹è¯ï¼Œè¿”å›æ¨¡å‹çš„å›å¤ã€‚
    è¯¥å‡½æ•°ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€‚

    :param user_question: ç”¨æˆ·çš„é—®é¢˜æˆ–æŒ‡ä»¤
    :param session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    :return: æ¨¡å‹çš„å›å¤å†…å®¹ï¼ˆæµå¼ï¼‰
    """
    try:
        # 1.å…ˆé€šè¿‡ rag è¿›è¡Œæ£€ç´¢
        vectorstore = create_vector_store()

        vectorstore_results = vectorstore.similarity_search_with_score(
            query=user_question,
            k=config["rag"]["retrieval_top_k"],
        )

        message_history = create_chat_with_memory()

        if vectorstore_results is None or len(vectorstore_results) == 0:
            print("æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£, ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå›å¤")
        else:
            print(f"æ£€ç´¢åˆ° {len(vectorstore_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            contents = []
            for result in vectorstore_results:
                document, score = result
                print(f"  - ç›¸ä¼¼åº¦: {score:.4f}, å†…å®¹: {document.page_content[:100]}...")
                # åªä½¿ç”¨ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„æ–‡æ¡£
                if score >= config["rag"]["similarity_threshold"]:
                    contents.append(document.page_content)

            if contents:
                # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°ç”¨æˆ·é—®é¢˜ä¸­
                user_question = f"""
                    è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

                    {chr(10).join(f"æ–‡æ¡£{i + 1}: {content}" for i, content in enumerate(contents))}

                    ç”¨æˆ·é—®é¢˜ï¼š{user_question}

                    è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚
                """
                print(f"å·²å°†æ£€ç´¢åˆ°çš„ {len(contents)} ä¸ªæ–‡æ¡£æ·»åŠ åˆ°æç¤ºè¯ä¸­")

        return message_history.stream(
            {"user_question": user_question},
            config=RunnableConfig(configurable={"session_id": session_id})
        )

    except Exception as e:
        print(f"å¯¹è¯è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        # å¦‚æœæ£€ç´¢å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹
        message_history = create_chat_with_memory()
        return message_history.stream(
            {"user_question": user_question},
            config=RunnableConfig(configurable={"session_id": session_id})
        )


def create_chat_with_memory() -> RunnableWithMessageHistory:
    """åˆ›å»ºä¸€ä¸ªå¯è®°å¿†å†å²çš„èŠå¤©å‡½æ•°"""
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", config["prompts"]["system"]),
        # å¯¹è¯å†å²
        MessagesPlaceholder(variable_name="history"),
        ("human", "{user_question}"),
    ])

    llm = get_large_model()
    runnable = prompt_template | llm

    message_history = RunnableWithMessageHistory(
        runnable=runnable,
        get_session_history=get_memory_history,
        input_messages_key="user_question",
        history_messages_key="history",
    )

    return message_history


def get_memory_history(session_id: str) -> ChatMessageHistory:
    """
    è·å–ä¼šè¯å†å²è®°å½•

    :param session_id: ä¼šè¯IDï¼Œç”¨äºå”¯ä¸€æ ‡è¯†ä¸€ä¸ªä¼šè¯
    :return: ä¼šè¯å†å²è®°å½•å¯¹è±¡ï¼ŒåŒ…å«è¯¥ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯
    """
    memory_history = chat_memory_history.get(session_id)
    if not memory_history:
        memory_history = ChatMessageHistory()
        chat_memory_history[session_id] = memory_history
    return memory_history


def get_large_model() -> ChatOpenAI:
    """è·å–å¤§æ¨¡å‹å®ä¾‹"""
    return ChatOpenAI(
        base_url=config["llm"]["base_url"],
        api_key=config["llm"]["api_key"],
        model=config["llm"]["model"],
        temperature=config["llm"]["temperature"],
        max_tokens=config["llm"]["max_tokens"],
    )


def rag_execute_with_file(path: str, session_id: str) -> Milvus:
    """
    æ‰§è¡ŒRAGæ¨¡å‹çš„ä¸€æ¬¡å¯¹è¯ï¼Œè¿”å›æ¨¡å‹çš„å›å¤ã€‚

    :param path: æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶ç›®å½•
    :param session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å¯¹è¯å†å²
    :return: å‘é‡å­˜å‚¨å¯¹è±¡
    """
    print(f"ç”¨æˆ·: {session_id}, å¼€å§‹å¤„ç†æ–‡ä»¶ç›®å½•: {path}")
    if not os.path.isdir(path):
        raise FileNotFoundError(f"æ–‡ä»¶ç›®å½• {path} ä¸å­˜åœ¨")

    file_list = os.listdir(path)
    if not file_list:
        raise FileNotFoundError(f"ç›®å½• {path} ä¸‹æ²¡æœ‰æ–‡ä»¶")

    documents = []
    for file in file_list:
        # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        file_path = os.path.join(path, file)
        if not os.path.isfile(file_path):
            continue

        # æ”¯æŒ .txt æ–‡ä»¶
        if file.endswith(".txt"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    if content.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                        documents.append(content)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
                continue

        # å¯ä»¥æ‰©å±•æ”¯æŒå…¶ä»–æ–‡ä»¶æ ¼å¼
        # elif file.endswith(".pdf"):
        #     # å¤„ç† PDF æ–‡ä»¶
        #     pass
        # elif file.endswith(".docx"):
        #     # å¤„ç† Word æ–‡ä»¶
        #     pass

    if not documents:
        raise ValueError(f"ç›®å½• {path} ä¸‹æ²¡æœ‰å¯å¤„ç†çš„æ–‡æœ¬å†…å®¹")

    print(f"å…±è¯»å– {len(documents)} ä¸ªæ–‡æ¡£")

    # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨å°†é•¿æ–‡æ¡£åˆ†å‰²æˆå°å—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config["text_splitter"]["chunk_size"],
        chunk_overlap=config["text_splitter"]["chunk_overlap"],
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""],
    )

    split_documents = []
    for doc in documents:
        splits = text_splitter.split_text(doc)
        split_documents.extend(splits)

    print(f"æ–‡æ¡£åˆ†å‰²åå…± {len(split_documents)} ä¸ªæ–‡æœ¬å—")

    # langchain å†…éƒ¨ä¼šè‡ªå·±åˆ›å»ºå’Œç®¡ç† milvus è¿æ¥
    # from_texts ä¼šè‡ªåŠ¨å®Œæˆï¼šæ–‡æœ¬ -> å‘é‡åŒ– -> å­˜å‚¨
    embeddings = get_embedding_model()
    try:
        vectorstore = Milvus.from_texts(
            texts=split_documents,
            embedding=embeddings,
            collection_name=config["milvus"]["collection_name"],
            connection_args={
                **connection_args,
                "db_name": config["milvus"]["db_name"],
            },
            drop_old=True  # å¦‚æœé›†åˆå­˜åœ¨åˆ™åˆ é™¤ï¼Œé‡æ–°åˆ›å»º
        )

        print(f"âœ… æˆåŠŸå­˜å‚¨ {len(split_documents)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")

    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}")
        raise
    finally:
        # æ–­å¼€è¿æ¥
        connections.disconnect("default")

    return vectorstore


def create_vector_store() -> Milvus:
    """
    åˆ›å»ºå‘é‡å­˜å‚¨
    :return: å‘é‡å­˜å‚¨å¯¹è±¡
    """
    try:
        db_name = config["milvus"]["db_name"]

        # å…ˆæ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        if not check_data_base_exists(db_name):
            raise ConnectionError("Milvus æ•°æ®åº“ä¸å­˜åœ¨")

        # langchain å†…éƒ¨é»˜è®¤ä¼šè¿›è¡Œ milvus çš„è¿æ¥
        vectorstore = Milvus(
            collection_name=config["milvus"]["collection_name"],
            connection_args={
                **connection_args,
                "db_name": db_name,
            },
            embedding_function=get_embedding_model(),
        )

        return vectorstore
    except Exception as e:
        print(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
        print("å°†ä½¿ç”¨ä¸å¸¦ RAG çš„æ¨¡å¼è¿›è¡Œå¯¹è¯")
        raise


def check_data_base_exists(db_name: str) -> bool:
    try:
        connections.connect(
            alias="default",
            **connection_args
        )

        database_list = db.list_database()
        if db_name in database_list:
            print(f"æ•°æ®åº“ {db_name} å·²å­˜åœ¨")
        else:
            db.create_database(db_name)
            print(f"æ•°æ®åº“ {db_name} åˆ›å»ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"æ£€æŸ¥ Milvus æ•°æ®åº“æ˜¯å¦å­˜åœ¨å¤±è´¥: {str(e)}")
        return False
    finally:
        # å…³é—­å»ºç«‹çš„è¿æ¥
        connections.disconnect("default")


def get_embedding_model() -> Embeddings:
    """
    è·å– Embedding æ¨¡å‹å®ä¾‹

    :return: Embedding æ¨¡å‹å¯¹è±¡
    """
    return CustomMultimodalEmbeddings(
        api_base=config["embedding"]["api_base"],
        api_key=config["embedding"]["api_key"],
        model=config["embedding"]["model"],
    )


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    session_id = "test_001"

    # æµ‹è¯•æ–‡ä»¶å¤„ç†
    path = os.path.join(os.path.dirname(__file__), "uploads")
    try:
        vectorstore = rag_execute_with_file(path, session_id)
        print("âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")

        results_with_scores = vectorstore.similarity_search_with_score("å¦‚ä½•ä»Javaè½¬æ¢åˆ°Pythonçš„å­¦ä¹ ", 3)
        if results_with_scores:
            for j, (doc, score) in enumerate(results_with_scores, 1):
                print(f"\n  ç»“æœ {j}:")
                print(f"  ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
                print(f"  ğŸ“„ å†…å®¹: {doc.page_content}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

    # æµ‹è¯•å¯¹è¯
    # user_questions = ["ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"]
    # for user_question in user_questions:
    #     print(f"\nç”¨æˆ·: {user_question}")
    #     print("AI: ", end="")
    #     response = chat_with_memory(user_question, session_id)
    #     for chunk in response:
    #         if hasattr(chunk, 'content'):
    #             print(chunk.content, end="", flush=True)
    #     print("\n")


if __name__ == "__main__":
    main()
