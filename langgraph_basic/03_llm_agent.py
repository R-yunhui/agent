"""
LangGraph åŸºç¡€ç¤ºä¾‹3ï¼šLLM Agent
æ¼”ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªå¸¦LLMè°ƒç”¨çš„æ™ºèƒ½ä»£ç†
"""
import os
from typing import TypedDict, Annotated, Sequence
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
import operator

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# 1. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    """AgentçŠ¶æ€"""
    messages: Annotated[Sequence[BaseMessage], operator.add]  # æ¶ˆæ¯å†å²
    next_action: str  # ä¸‹ä¸€æ­¥åŠ¨ä½œ


# 2. åˆ›å»ºLLM
llm = ChatOpenAI(
    model=os.getenv("OPENAI_CHAT_MODEL"),
    base_url=os.getenv("OPENAI_API_BASE_URL"),
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY")
)


# 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def call_model(state: AgentState) -> AgentState:
    """è°ƒç”¨LLMèŠ‚ç‚¹ - ä½¿ç”¨æµå¼è¾“å‡º"""
    messages = state['messages']
    
    # æ·»åŠ ç³»ç»Ÿæç¤º
    system_message = SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚
è¯·ç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¯´å†è§æˆ–ç»“æŸå¯¹è¯ï¼Œåœ¨å›å¤ä¸­åŒ…å«'å†è§'ã€‚""")
    
    full_messages = [system_message] + list(messages)
    
    # åœ¨æ­¤å¤„ç›´æ¥è¿›è¡Œæµå¼æ‰“å°ï¼Œå®ç°çœŸæ­£çš„é€å­—è¾“å‡ºæ•ˆæœ
    print("\nğŸ¤– AI: ", end="", flush=True)
    response_content = ""
    for chunk in llm.stream(full_messages):
        print(chunk.content, end="", flush=True)
        response_content += chunk.content
    print()  # æ¢è¡Œ
    
    response = AIMessage(content=response_content)
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯
    if any(word in response.content.lower() for word in ['å†è§', 'goodbye', 'æ‹œæ‹œ']):
        state['next_action'] = 'end'
    else:
        state['next_action'] = 'continue'
    
    return {
        "messages": [response],
        "next_action": state['next_action']
    }


def should_continue(state: AgentState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­å¯¹è¯"""
    return state['next_action']


# 4. åˆ›å»ºAgentå›¾
def create_agent_graph():
    """åˆ›å»ºLLM Agentå›¾"""
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("agent", call_model)
    
    # è®¾ç½®å…¥å£
    workflow.set_entry_point("agent")
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´å¤æ‚ï¼‰
    workflow.add_edge("agent", END)
    
    return workflow.compile()


# 5. è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    app = create_agent_graph()
    
    print("=" * 60)
    print("LangGraph LLM Agent ç¤ºä¾‹")
    print("æç¤ºï¼šè¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¶ˆæ¯å†å²
    message_history = []
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("ç¨‹åºé€€å‡º")
            break
        
        if not user_input:
            continue
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        message_history.append(HumanMessage(content=user_input))
        
        # è¿è¡Œå›¾ï¼ˆæµå¼è¾“å‡ºåœ¨ call_model å†…éƒ¨å·²å®Œæˆï¼‰
        result = app.invoke({
            "messages": message_history,
            "next_action": "continue"
        })
        
        # æ›´æ–°æ¶ˆæ¯å†å²
        message_history.extend(result['messages'])
        
        # å¦‚æœAIè¯´å†è§ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
        if result.get('next_action') == 'end':
            cont = input("\næ˜¯å¦ç»§ç»­å¯¹è¯ï¼Ÿ(y/n): ").strip().lower()
            if cont != 'y':
                print("å¯¹è¯ç»“æŸ")
                break
